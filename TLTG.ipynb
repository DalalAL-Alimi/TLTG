{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnPT5GgSfPYo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "import random as rn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "np.random.seed(1)\n",
        "rn.seed(1)\n",
        "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
        "\n",
        "from keras import backend as K\n",
        "tf.set_random_seed(1)\n",
        "\n",
        "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
        "tf.keras.backend.set_session(sess)\n",
        "\n",
        "from pandas import DataFrame\n",
        "from pandas import Series\n",
        "from pandas import concat\n",
        "from pandas import read_csv\n",
        "from pandas import datetime\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras.layers import LSTM,GRU\n",
        "from keras.constraints import max_norm\n",
        "from math import sqrt\n",
        "from matplotlib import pyplot\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from numpy import concatenate\n",
        "from sklearn.metrics import r2_score\n",
        "import time\n",
        "def parser(x):\n",
        "  return datetime.strptime(x, '%Y%m')\n",
        "\n",
        "# create a differenced series\n",
        "def difference(dataset, interval=1):\n",
        "  diff = list()\n",
        "  for i in range(interval, len(dataset)):\n",
        "    value = dataset[i] - dataset[i - interval]\n",
        "    diff.append(value)\n",
        "  return Series(diff)\n",
        "\n",
        "# invert differenced value\n",
        "def inverse_difference(history, yhat, interval=1):\n",
        "  return yhat + history[-interval]\n",
        "\n",
        "# scale train and test data to [-1, 1]\n",
        "def scale(train, test):\n",
        "  # fit scaler\n",
        "  scaler = QuantileTransformer(n_quantiles=15, random_state=0,output_distribution='normal') #\n",
        "  scaler = scaler.fit(train)\n",
        "  \n",
        "  # transform train\n",
        "  train = train.reshape(train.shape[0], train.shape[1])\n",
        "  train_scaled = scaler.fit_transform(train)\n",
        "  # transform test\n",
        "  test = test.reshape(test.shape[0], test.shape[1])\n",
        "  test_scaled = scaler.fit_transform(test)\n",
        "  return scaler, train_scaled, test_scaled\n",
        "\n",
        "# inverse scaling for a forecasted value\n",
        "def invert_scale(scaler, X, value):\n",
        "  new_row = [x for x in X] + [value]\n",
        "  array = np.array(new_row)\n",
        "  array = array.reshape(1, len(array))\n",
        "  inverted = scaler.inverse_transform(array)\n",
        "  return inverted[0, -1]\n",
        "\n",
        "# fit an LSTM network to training data\n",
        "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
        "  X, y = train[:, 0:-1], train[:, -1]\n",
        "  X = X.reshape(X.shape[0], X.shape[1],1 )\n",
        "  model1 = Sequential()\n",
        "  model1.add(LSTM(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]) , stateful=True,return_sequences = True))\n",
        "  model1.add(LSTM(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences = True))\n",
        "\n",
        "  # Freeze the base model\n",
        "  model1.trainable = False\n",
        "  model = Sequential()\n",
        "  model.add(model1)\n",
        "\n",
        "  model.add(GRU(neurons[0], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences=True))\n",
        "  model.add(GRU(neurons[1], batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True,return_sequences=True))\n",
        "\n",
        "  model.add(Dense(10, activation='elu'))\n",
        "  model.add(Dense(1))\n",
        "       \n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "  print(model.summary())\n",
        " \n",
        "  for i in range(nb_epoch):\n",
        "    print('Epoch:',i)\n",
        "    model.fit(X, y, epochs=1, batch_size=batch_size, verbose=1, shuffle=False)\n",
        "    model.reset_states()\n",
        "  return model\n",
        "  \n",
        "\n",
        "# make a one-step forecast\n",
        "def forecast_lstm(model, batch_size, X):\n",
        "  X = X.reshape(1, len(X), 1)\n",
        "  yhat = model.predict(X, batch_size=batch_size)\n",
        "  return yhat[0,0]\n",
        "\n",
        "# Update LSTM model\n",
        "def update_model(model, train, batch_size, updates):\n",
        "  X, y = train[:, 0:-1], train[:, -1]\n",
        "  X = X.reshape(X.shape[0], X.shape[1],1 )\n",
        "  for i in range(updates):\n",
        "    model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
        "    model.reset_states()\n",
        "\n",
        "# convert an array of values into a dataset matrix\n",
        "def create_dataset(dataset, look_back=1):\n",
        "  dataset = np.insert(dataset,[0]*look_back,0)    \n",
        "  dataX, dataY = [], []\n",
        "  for i in range(len(dataset)-look_back):\n",
        "    a = dataset[i:(i+look_back)]\n",
        "    dataX.append(a)\n",
        "    dataY.append(dataset[i + look_back])\n",
        "  dataY= np.array(dataY)        \n",
        "  dataY = np.reshape(dataY,(dataY.shape[0],1))\n",
        "  dataset = np.concatenate((dataX,dataY),axis=1)  \n",
        "  return dataset\n",
        "\n",
        "# compute RMSPE\n",
        "def RMSPE(x,y):\n",
        "  result=0\n",
        "  for i in range(len(x)):\n",
        "    result += ((x[i]-y[i])/x[i])**2\n",
        "  result /= len(x)\n",
        "  result = sqrt(result)\n",
        "  result *= 100\n",
        "  return result\n",
        "\n",
        "# compute MAPE\n",
        "def MAPE(x,y):\n",
        "  result=0\n",
        "  for i in range(len(x)):\n",
        "    result += abs((x[i]-y[i])/x[i])\n",
        "  result /= len(x)\n",
        "  result *= 100\n",
        "  return result\n",
        "\n",
        "\n",
        "\n",
        "def experiment(series, updates,look_back,neurons,n_epoch):\n",
        "\n",
        "  raw_values = series.values\n",
        "  # transform data to be stationary\n",
        "  diff = difference(raw_values, 1)\n",
        "\n",
        "  # create dataset x,y\n",
        "  dataset = diff.values\n",
        "  dataset = create_dataset(dataset,look_back)\n",
        "\n",
        "  # split into train and test sets\n",
        "  #train_size = int(dataset.shape[0]) - 48 # For 48 samples for testing \n",
        "  train_size = int(dataset.shape[0] * 0.8) # For %20 of the dataset for testing \n",
        "\n",
        "  test_size = dataset.shape[0] - train_size\n",
        "  train, test = dataset[0:train_size], dataset[train_size:]\n",
        "\n",
        "  # transform the scale of the data\n",
        "  scaler, train_scaled, test_scaled = scale(train, test)\n",
        "\n",
        "  # fit the model\n",
        "  training_start_time = time.time()\n",
        "\n",
        "  lstm_model = fit_lstm(train_scaled, 1, n_epoch, neurons)\n",
        "    \n",
        "  training_end_time = time.time()\n",
        "\n",
        "  # forecast the entire training dataset to build up state for forecasting\n",
        "  print('Forecasting Training Data')   \n",
        "  predictions_train = list()\n",
        "    \n",
        "  for i in range(len(train_scaled)):\n",
        "    # make one-step forecast\n",
        "    X, y = train_scaled[i, 0:-1], train_scaled[i, -1]\n",
        "    yhat = forecast_lstm(lstm_model, 1, X)\n",
        "    # invert scaling\n",
        "    yhat = invert_scale(scaler, X, yhat)\n",
        "    # invert differencing\n",
        "    yhat = inverse_difference(raw_values, yhat, len(raw_values)-i)\n",
        "    # store forecast\n",
        "    predictions_train.append(yhat)\n",
        "    expected = raw_values[ i+1 ] \n",
        "\n",
        "\n",
        "\n",
        "  # forecast the test data\n",
        "  print('Forecasting Testing Data')\n",
        "  train_copy = np.copy(train_scaled)\n",
        "  predictions_test = list()\n",
        "  test_start_time = time.time()\n",
        "\n",
        "  for i in range(len(test_scaled)):\n",
        "    # update model\n",
        "    #if i > 0:\n",
        "      #update_model(lstm_model, train_copy, 1, updates)\n",
        "    # make one-step forecast\n",
        "    X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
        "    yhat = forecast_lstm(lstm_model, 1, X)\n",
        "    # invert scaling\n",
        "    yhat = invert_scale(scaler, X, yhat)\n",
        "    # invert differencing\n",
        "    yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
        "    # store forecast\n",
        "    predictions_test.append(yhat)\n",
        "    # add to training set\n",
        "    train_copy = concatenate((train_copy, test_scaled[i,:].reshape(1, -1)))\n",
        "    expected = raw_values[len(train) + i + 1]\n",
        "\n",
        "  test_end_time = time.time()\n",
        "\n",
        "    \n",
        "  # report performance\n",
        "  print(\"Training time : \", training_end_time - training_start_time )\n",
        "  print(\"Testing time : \", test_end_time - test_start_time)\n",
        "    \n",
        "  rmse_train = sqrt(mean_squared_error(raw_values[:len(train_scaled),:], predictions_train))\n",
        "  print('Train RMSE: %.3f' % rmse_train)\n",
        "  #report performance using RMSPE\n",
        "  rmspe_train = RMSPE(raw_values[:len(train_scaled),:],predictions_train)\n",
        "  print('Train RMSPE: %.3f' % rmspe_train)\n",
        "  MAE_train = mean_absolute_error(raw_values[:len(train_scaled),:], predictions_train)\n",
        "  print('Train MAE: %.5f' % MAE_train)\n",
        "  MAPE_train = MAPE(raw_values[:len(train_scaled),:], predictions_train)\n",
        "  print('Train MAPE: %.5f' % MAPE_train)\n",
        "  r2 = r2_score(raw_values[:len(train_scaled),:], predictions_train)\n",
        "  print('r2 score for training model is', r2)\n",
        "    # report performance using RMSE\n",
        "  print(\"raw_values = \", raw_values.shape,test_scaled.shape[0])\n",
        " \n",
        "  dd = raw_values.shape[0]-1\n",
        "\n",
        "  rmse_test = sqrt(mean_squared_error(raw_values[train_scaled.shape[0]:dd,:], predictions_test))\n",
        "  print('Test RMSE: %.3f' % rmse_test)\n",
        "  #report performance using RMSPE\n",
        "  rmspe_test = RMSPE(raw_values[train_scaled.shape[0]:dd,:],predictions_test)\n",
        "  print('Test RMSPE: %.3f' % rmspe_test)\n",
        "  MAE_test = mean_absolute_error(raw_values[train_scaled.shape[0]:dd,:], predictions_test)\n",
        "  print('Test MAE: %.5f' % MAE_test)\n",
        "  MAPE_test = MAPE(raw_values[train_scaled.shape[0]:dd,:], predictions_test)\n",
        "  print('Test MAPE: %.5f' % MAPE_test)\n",
        "  r22 = r2_score(raw_values[train_scaled.shape[0]:dd,:], predictions_test)\n",
        "  print('r2 score for testing model is', r22)\n",
        "\n",
        "  predictions = np.concatenate((predictions_train,predictions_test),axis=0)\n",
        "\n",
        "  fc_report = pd.DataFrame(predictions_test)\n",
        "  fc_report.to_csv('Qt AirPassengers_TLTG.csv', index= True)\n",
        "  fc_report2 = pd.DataFrame(predictions)\n",
        "  fc_report2.to_csv('All Qt AirPassengers_TLTG.csv', index= True)   \n",
        "    # line plot of observed vs predicted\n",
        "  fig, ax = plt.subplots(1)\n",
        "  ax.plot(raw_values, label='original', color='blue')\n",
        "  ax.plot(predictions, label='predictions', color='red')\n",
        "  ax.axvline(x=len(train_scaled)+1,color='k', linestyle='--')\n",
        "  ax.legend(loc='upper right')\n",
        "  ax.set_xlabel(\"Time\",fontsize = 16)\n",
        "  ax.set_ylabel('oil production',fontsize = 16)\n",
        "  plt.show()\n",
        "   \n",
        "   \n",
        "\n",
        "def run():\n",
        "\n",
        "  #load dataset\n",
        "  series = read_csv('AirPassengers.csv')\n",
        "  look_back= 60\n",
        "  neurons= [ 1 , 1 ]\n",
        "  n_epochs=10\n",
        "  updates= 1\n",
        "  \n",
        "\n",
        "  experiment(series, updates,look_back,neurons,n_epochs)\n",
        "\n",
        "run()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhTQyWJPfhR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}